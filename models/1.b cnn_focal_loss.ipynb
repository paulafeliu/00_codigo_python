{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import cv2\n",
    "import os\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from PIL import Image\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from glob import glob\n",
    "from skimage.io import imread\n",
    "from os import listdir\n",
    "import time\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = F.cross_entropy(input, target, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        if self.alpha is not None:\n",
    "            focal_loss = self.alpha * focal_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281/281 [00:02<00:00, 120.81it/s]\n",
      "5000it [00:02, 2441.88it/s]\n",
      "5000it [00:02, 2417.11it/s]\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"/home/xnmaster/dataset\"  # Ruta del directorio de imágenes\n",
    "\n",
    "N_IDC = []\n",
    "P_IDC = []\n",
    "\n",
    "for dir_name in tqdm(os.listdir(root_dir)):\n",
    "    dir_path = os.path.join(root_dir, dir_name)\n",
    "    if os.path.isdir(dir_path):\n",
    "        negative_dir_path = os.path.join(dir_path, '0')\n",
    "        positive_dir_path = os.path.join(dir_path, '1')\n",
    "        if os.path.isdir(negative_dir_path) and os.path.isdir(positive_dir_path):\n",
    "            negative_image_paths = [\n",
    "                os.path.join(negative_dir_path, image_name)\n",
    "                for image_name in os.listdir(negative_dir_path)\n",
    "                if image_name.endswith('.png')\n",
    "            ]\n",
    "            positive_image_paths = [\n",
    "                os.path.join(positive_dir_path, image_name)\n",
    "                for image_name in os.listdir(positive_dir_path)\n",
    "                if image_name.endswith('.png')\n",
    "            ]\n",
    "            N_IDC.extend(negative_image_paths)\n",
    "            P_IDC.extend(positive_image_paths)\n",
    "\n",
    "total_images = 50000  # Cambiado a 5000 para equilibrar las clases (2500 benignos y 2500 malignos)\n",
    "\n",
    "n_img_arr = np.zeros(shape=(total_images, 50, 50, 3), dtype=np.float32)\n",
    "p_img_arr = np.zeros(shape=(total_images, 50, 50, 3), dtype=np.float32)\n",
    "label_n = np.zeros(total_images)\n",
    "label_p = np.ones(total_images)\n",
    "\n",
    "for i, img in tqdm(enumerate(N_IDC[:total_images])):\n",
    "    n_img = cv2.imread(img, cv2.IMREAD_COLOR)\n",
    "    n_img_size = cv2.resize(n_img, (50, 50), interpolation=cv2.INTER_LINEAR)\n",
    "    n_img_arr[i] = n_img_size\n",
    "\n",
    "for i, img in tqdm(enumerate(P_IDC[:total_images])):\n",
    "    p_img = cv2.imread(img, cv2.IMREAD_COLOR)\n",
    "    p_img_size = cv2.resize(p_img, (50, 50), interpolation=cv2.INTER_LINEAR)\n",
    "    p_img_arr[i] = p_img_size\n",
    "\n",
    "X = np.concatenate((p_img_arr, n_img_arr), axis=0)\n",
    "y = np.concatenate((label_p, label_n), axis=0)\n",
    "X, y = shuffle(X, y, random_state=0)\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, output_size=2):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=4, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout1 = nn.Dropout2d(p=0.3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(12 * 12 * 64, 64)\n",
    "        self.bn5 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "        self.fc3 = nn.Linear(64, 24)\n",
    "        self.fc4 = nn.Linear(24, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN().to(device)\n",
    "criterion = FocalLoss(gamma=2, alpha=None)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    return train_loss\n",
    "\n",
    "def evaluate(model, device, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = 100.0 * correct / len(val_loader.dataset)\n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00, Train Loss: 0.0913, Val Loss: 0.1486, Val Accuracy: 81.28%\n",
      "Epoch: 01, Train Loss: 0.0740, Val Loss: 0.1635, Val Accuracy: 69.17%\n",
      "Epoch: 02, Train Loss: 0.0746, Val Loss: 0.0891, Val Accuracy: 86.17%\n",
      "Epoch: 03, Train Loss: 0.0636, Val Loss: 0.0630, Val Accuracy: 90.22%\n",
      "Epoch: 04, Train Loss: 0.0670, Val Loss: 0.6854, Val Accuracy: 62.39%\n",
      "Epoch: 05, Train Loss: 0.0645, Val Loss: 0.0781, Val Accuracy: 86.78%\n",
      "Epoch: 06, Train Loss: 0.0591, Val Loss: 0.4326, Val Accuracy: 50.33%\n",
      "Epoch: 07, Train Loss: 0.0584, Val Loss: 0.0691, Val Accuracy: 87.72%\n",
      "Epoch: 08, Train Loss: 0.0514, Val Loss: 0.4015, Val Accuracy: 64.94%\n",
      "Epoch: 09, Train Loss: 0.0499, Val Loss: 0.0593, Val Accuracy: 90.72%\n",
      "Epoch: 10, Train Loss: 0.0475, Val Loss: 0.1264, Val Accuracy: 81.06%\n",
      "Epoch: 11, Train Loss: 0.0486, Val Loss: 0.1976, Val Accuracy: 76.06%\n",
      "Epoch: 12, Train Loss: 0.0446, Val Loss: 0.2016, Val Accuracy: 77.44%\n",
      "Epoch: 13, Train Loss: 0.0458, Val Loss: 1.3397, Val Accuracy: 65.78%\n",
      "Epoch: 14, Train Loss: 0.0482, Val Loss: 0.1245, Val Accuracy: 83.67%\n",
      "Epoch: 15, Train Loss: 0.0401, Val Loss: 0.0441, Val Accuracy: 93.89%\n",
      "Epoch: 16, Train Loss: 0.0400, Val Loss: 0.0448, Val Accuracy: 93.17%\n",
      "Epoch: 17, Train Loss: 0.0390, Val Loss: 0.6140, Val Accuracy: 66.50%\n",
      "Epoch: 18, Train Loss: 0.0380, Val Loss: 0.3373, Val Accuracy: 58.83%\n",
      "Epoch: 19, Train Loss: 0.0384, Val Loss: 0.0530, Val Accuracy: 92.89%\n",
      "Epoch: 20, Train Loss: 0.0337, Val Loss: 0.0838, Val Accuracy: 89.17%\n",
      "Epoch: 21, Train Loss: 0.0311, Val Loss: 0.2207, Val Accuracy: 72.50%\n",
      "Epoch: 22, Train Loss: 0.0279, Val Loss: 0.1713, Val Accuracy: 76.22%\n",
      "Epoch: 23, Train Loss: 0.0280, Val Loss: 0.1176, Val Accuracy: 88.94%\n",
      "Epoch: 24, Train Loss: 0.0252, Val Loss: 0.0877, Val Accuracy: 88.94%\n",
      "Epoch: 25, Train Loss: 0.0230, Val Loss: 0.0721, Val Accuracy: 91.89%\n",
      "Epoch: 26, Train Loss: 0.0275, Val Loss: 0.1047, Val Accuracy: 84.67%\n",
      "Epoch: 27, Train Loss: 0.0229, Val Loss: 0.0892, Val Accuracy: 89.17%\n",
      "Epoch: 28, Train Loss: 0.0219, Val Loss: 0.0950, Val Accuracy: 88.72%\n",
      "Epoch: 29, Train Loss: 0.0204, Val Loss: 0.2640, Val Accuracy: 82.67%\n",
      "Epoch: 30, Train Loss: 0.0209, Val Loss: 0.1132, Val Accuracy: 89.33%\n",
      "Epoch: 31, Train Loss: 0.0175, Val Loss: 0.0749, Val Accuracy: 92.39%\n",
      "Epoch: 32, Train Loss: 0.0196, Val Loss: 0.1183, Val Accuracy: 85.50%\n",
      "Epoch: 33, Train Loss: 0.0226, Val Loss: 0.0819, Val Accuracy: 90.94%\n",
      "Epoch: 34, Train Loss: 0.0205, Val Loss: 0.1549, Val Accuracy: 83.39%\n",
      "Epoch: 35, Train Loss: 0.0153, Val Loss: 0.1863, Val Accuracy: 85.89%\n",
      "Epoch: 36, Train Loss: 0.0151, Val Loss: 0.1524, Val Accuracy: 84.94%\n",
      "Epoch: 37, Train Loss: 0.0130, Val Loss: 0.0944, Val Accuracy: 91.94%\n",
      "Epoch: 38, Train Loss: 0.0124, Val Loss: 0.0868, Val Accuracy: 93.06%\n",
      "Epoch: 39, Train Loss: 0.0157, Val Loss: 0.0810, Val Accuracy: 92.44%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32  # Define the batch size\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train).float().permute(0, 3, 1, 2), torch.from_numpy(y_train).long())\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_val).float().permute(0, 3, 1, 2), torch.from_numpy(y_val).long())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_epochs = 40\n",
    "best_val_loss = float('inf')\n",
    "losses = {\"train\": [], \"val\": []}\n",
    "hist = {\"train\": [], \"val\": []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = train(model, device, train_loader, optimizer, criterion)\n",
    "    model.eval()\n",
    "    val_loss, val_accuracy = evaluate(model, device, val_loader, criterion)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    losses[\"train\"].append(train_loss)\n",
    "    losses[\"val\"].append(val_loss)\n",
    "    hist[\"train\"].append(val_accuracy)\n",
    "    hist[\"val\"].append(val_accuracy)\n",
    "\n",
    "    print('Epoch: {:02d}, Train Loss: {:.4f}, Val Loss: {:.4f}, Val Accuracy: {:.2f}%'.format(\n",
    "        epoch, train_loss, val_loss, val_accuracy))\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses and accuracies\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.plot(losses[\"train\"], label=\"training loss\")\n",
    "ax1.plot(losses[\"val\"], label=\"validation loss\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(hist[\"train\"], label=\"training accuracy\")\n",
    "ax2.plot(hist[\"val\"], label=\"validation accuracy\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test).float().permute(0, 3, 1, 2), torch.from_numpy(y_test).long())\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loss, test_accuracy = evaluate(model, device, test_loader, criterion)\n",
    "\n",
    "print('Test Loss: {:.4f}, Test Accuracy: {:.2f}%'.format(test_loss, test_accuracy))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate over the test data and make predictions\n",
    "list_img_names = []\n",
    "\n",
    "counter = 0\n",
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    for j in range(inputs.size(0)):\n",
    "        # Obtener el nombre de la imagen\n",
    "        image_index = i * test_loader.batch_size + j\n",
    "        image_path = N_IDC[image_index] if preds[j].item() == 0 else P_IDC[image_index]\n",
    "        image_name = os.path.basename(image_path)\n",
    "        print(\"Nombre de la imagen: {}\".format(image_name))\n",
    "        list_img_names.append(image_name)\n",
    "        # Loading and showing the image\n",
    "        image = inputs[j].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        # Normalizing the image\n",
    "        image = (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "        \"\"\"\n",
    "        plt.figure()\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        # Print the prediction and the correct label\n",
    "        prediction = preds[j].item()\n",
    "        correct_label = labels[j].item()\n",
    "        print(\"Predicción: {}, Etiqueta correcta: {}\".format(prediction, correct_label), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = []\n",
    "for name in list_img_names:\n",
    "    id = name.split('_')[0]\n",
    "    patient_ids.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_unique = list(set(patient_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cancer_dataframe(patient_id, cancer_id):\n",
    "    path = os.path.join(base_path, patient_id, cancer_id)\n",
    "    files = os.listdir(path)\n",
    "    dataframe = pd.DataFrame(files, columns=[\"filename\"])\n",
    "    path_names = [os.path.join(path, filename) for filename in dataframe.filename.values]\n",
    "    dataframe = dataframe.filename.str.rsplit(\"_\", n=4, expand=True)\n",
    "    dataframe.loc[:, \"target\"] = np.int(cancer_id)\n",
    "    dataframe.loc[:, \"path\"] = path_names\n",
    "    dataframe = dataframe.drop([0, 1, 4], axis=1)\n",
    "    dataframe = dataframe.rename({2: \"x\", 3: \"y\"}, axis=1)\n",
    "    dataframe.loc[:, \"x\"] = dataframe.loc[:,\"x\"].str.replace(\"x\", \"\", case=False).astype(np.int)\n",
    "    dataframe.loc[:, \"y\"] = dataframe.loc[:,\"y\"].str.replace(\"y\", \"\", case=False).astype(np.int)\n",
    "    return dataframe\n",
    "\n",
    "def get_patient_dataframe(patient_id):\n",
    "    df_0 = get_cancer_dataframe(patient_id, \"0\")\n",
    "    df_1 = get_cancer_dataframe(patient_id, \"1\")\n",
    "    patient_df = pd.concat([df_0, df_1], ignore_index=True)\n",
    "    return patient_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_breast_tissue_base(patient_id, pred_df=None):\n",
    "    example_df = get_patient_dataframe(patient_id)\n",
    "    max_point = [example_df.y.max()-1, example_df.x.max()-1]\n",
    "    grid = 255*np.ones(shape=(max_point[0] + 50, max_point[1] + 50, 3)).astype(np.uint8)\n",
    "    mask = 255*np.ones(shape=(max_point[0] + 50, max_point[1] + 50, 3)).astype(np.uint8)\n",
    "    if pred_df is not None:\n",
    "        patient_df = pred_df[pred_df.patient_id == patient_id].copy()\n",
    "    mask_proba = np.zeros(shape=(max_point[0] + 50, max_point[1] + 50, 1)).astype(np.float)\n",
    "    \n",
    "    broken_patches = []\n",
    "    for n in range(len(example_df)):\n",
    "        try:\n",
    "            image = imread(example_df.path.values[n])\n",
    "            target = example_df.target.values[n]\n",
    "            x_coord = np.int(example_df.x.values[n])\n",
    "            y_coord = np.int(example_df.y.values[n])\n",
    "            x_start = x_coord - 1\n",
    "            y_start = y_coord - 1\n",
    "            x_end = x_start + 50\n",
    "            y_end = y_start + 50\n",
    "\n",
    "            grid[y_start:y_end, x_start:x_end] = image\n",
    "            if target == 1:\n",
    "                mask[y_start:y_end, x_start:x_end, 0] = 250\n",
    "                mask[y_start:y_end, x_start:x_end, 1] = 0\n",
    "                mask[y_start:y_end, x_start:x_end, 2] = 0\n",
    "            if pred_df is not None:\n",
    "                proba = patient_df[(patient_df.x==x_coord) & (patient_df.y==y_coord)].proba\n",
    "                mask_proba[y_start:y_end, x_start:x_end, 0] = np.float(proba)\n",
    "\n",
    "        except ValueError:\n",
    "            broken_patches.append(example_df.path.values[n])\n",
    "    \n",
    "    return grid, mask, broken_patches, mask_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_breast_tissue(patient_id):\n",
    "    grid, mask, broken_patches,_ = visualise_breast_tissue_base(patient_id)\n",
    "\n",
    "    fig, ax = plt.subplots(1,2,figsize=(20,10))\n",
    "    ax[0].imshow(grid, alpha=0.9)\n",
    "    ax[1].imshow(mask, alpha=0.8)\n",
    "    ax[1].imshow(grid, alpha=0.7)\n",
    "    ax[0].grid(False)\n",
    "    ax[1].grid(False)\n",
    "    for m in range(2):\n",
    "        ax[m].set_xlabel(\"X-coord\")\n",
    "        ax[m].set_ylabel(\"Y-coord\")\n",
    "    ax[0].set_title(\"Breast tissue slice of patient: \" + patient_id)\n",
    "    ax[1].set_title(\"Cancer tissue colored red \\n of patient: \" + patient_id)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_breast_tissue_binary(patient_id):\n",
    "        \n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "    example_df = get_patient_dataframe(patient_id)\n",
    "\n",
    "    ax.scatter(example_df.x.values, example_df.y.values, c=example_df.target.values, cmap=\"coolwarm\", s=20)\n",
    "    ax.set_title(\"Patient \" + patient_id)\n",
    "    ax.set_xlabel(\"X coord\")\n",
    "    ax.set_ylabel(\"Y coord\")\n",
    "    ax.set_aspect('equal')  # Set aspect ratio to 'equal' to preserve original orientation\n",
    "    ax.invert_yaxis()  # Reverse the y-axis direction\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener 5 elementos aleatorios de la lista\n",
    "random_patient_ids = random.sample(patient_ids, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in random_patient_ids: #ids_unique\n",
    "    print(\"Patient's ID: \", id)\n",
    "    visualise_breast_tissue(id)\n",
    "    visualise_breast_tissue_binary(id)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
