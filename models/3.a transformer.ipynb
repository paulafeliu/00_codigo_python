{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de la carpeta principal\n",
    "main_folder = \"/home/xnmaster/dataset\"\n",
    "\n",
    "input_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Just normalization\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size para el entrenamiento (cambia según la cantidad de memoria disponible)\n",
    "batch_size = 8\n",
    "\n",
    "# Crear datasets de entrenamiento y validación\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(main_folder, x), data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "\n",
    "\"\"\"\n",
    "# Generar los índices para el subconjunto\n",
    "subset_indices_train = torch.randperm(len(image_datasets['train']))[:int(0.1*len(image_datasets['train']))]\n",
    "subset_indices_val = torch.randperm(len(image_datasets['val']))[:int(0.1*len(image_datasets['val']))]\n",
    "\n",
    "# Crear subconjuntos\n",
    "train_data_subset = Subset(image_datasets['train'], subset_indices_train)\n",
    "val_data_subset = Subset(image_datasets['val'], subset_indices_val)\n",
    "\n",
    "\n",
    "# Crear dataloaders de entrenamiento y validación\n",
    "dataloaders_dict = {\n",
    "    'train': DataLoader(train_data_subset, batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "    'val': DataLoader(val_data_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "}\n",
    "\"\"\"\n",
    "dataloaders_dict = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val','test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Define el modelo del Transformer con Attention y Positional Encoding\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_inputs, hidden_dim, num_classes, input_size):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.positional_encoding = self.generate_positional_encoding(input_size, input_dim)\n",
    "\n",
    "        self.query_fc = nn.Linear(input_dim, hidden_dim)\n",
    "        self.keys_fc = nn.Linear(input_dim, hidden_dim)\n",
    "        self.values_fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=4)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def generate_positional_encoding(self, input_size, hidden_dim):\n",
    "        positional_encoding = torch.zeros(1, input_size, hidden_dim)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * -(math.log(10000.0) / hidden_dim))\n",
    "        position = torch.arange(0, input_size).unsqueeze(1).float()\n",
    "        positional_encoding[:, :, 0::2] = torch.sin(position * div_term[:hidden_dim // 2])\n",
    "        if hidden_dim % 2 == 1:  # hidden_dim is odd\n",
    "            positional_encoding[:, :, 1::2] = torch.cos(position * div_term[:hidden_dim // 2])\n",
    "        else:  # hidden_dim is even\n",
    "            positional_encoding[:, :, 1::2] = torch.cos(position * div_term[1:hidden_dim // 2 + 1])\n",
    "        return positional_encoding.to(device)\n",
    "\n",
    "    def scoring_additive(self, query, keys):\n",
    "        query = query.repeat(1, self.num_inputs, 1)\n",
    "        query = torch.tanh(self.query_fc(query))\n",
    "\n",
    "        keys = torch.tanh(self.keys_fc(keys))\n",
    "\n",
    "        score = torch.tanh(query + keys)\n",
    "        score = self.values_fc(score)\n",
    "        return score\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_inputs, input_dim, _ = x.size()\n",
    "        x = x.view(batch_size, input_dim, -1) #x = x.view(batch_size, num_inputs * input_dim, -1)\n",
    "        positional_encoding = self.positional_encoding[:, :input_dim, :].repeat(batch_size, 1, 1)\n",
    "\n",
    "        keys = self.scoring_additive(x, positional_encoding)\n",
    "        keys = keys.view(batch_size, -1)\n",
    "\n",
    "        keys = self.encoder(keys.unsqueeze(-1)).squeeze()\n",
    "\n",
    "        keys = keys.view(batch_size, num_inputs, -1)\n",
    "\n",
    "        keys = keys.max(dim=1)[0]\n",
    "\n",
    "        out = self.classifier(keys)\n",
    "        return out\n",
    "\"\"\"\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_inputs, hidden_dim, num_classes):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.positional_encoding = self.generate_positional_encoding(self.num_inputs, self.hidden_dim)\n",
    "\n",
    "        self.query_fc = nn.Linear(input_dim, hidden_dim)\n",
    "        self.keys_fc = nn.Linear(input_dim, hidden_dim)\n",
    "        self.values_fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=4)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def generate_positional_encoding(self, input_size, hidden_dim):\n",
    "        positional_encoding = torch.zeros(input_size, hidden_dim)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * -(math.log(10000.0) / hidden_dim))\n",
    "        position = torch.arange(0, input_size).unsqueeze(1).float()\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term[:hidden_dim // 2])\n",
    "        if hidden_dim % 2 == 1:  # hidden_dim is odd\n",
    "            positional_encoding[:, 1::2] = torch.cos(position * div_term[:hidden_dim // 2])\n",
    "        else:  # hidden_dim is even\n",
    "            positional_encoding[:, 1::2] = torch.cos(position * div_term[1:hidden_dim // 2 + 1])\n",
    "\n",
    "        return positional_encoding.to(device)\n",
    "\n",
    "    def scoring_additive(self, query, keys):\n",
    "        query = query.repeat(1, self.num_inputs, 1)\n",
    "        query = torch.tanh(self.query_fc(query))\n",
    "\n",
    "        keys = torch.tanh(self.keys_fc(keys))\n",
    "\n",
    "        score = torch.tanh(query + keys)\n",
    "        score = self.values_fc(score)\n",
    "        return score\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, self.num_inputs, -1)\n",
    "\n",
    "        positional_encoding = self.positional_encoding.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "        x = x + positional_encoding\n",
    "\n",
    "        scores = self.scoring_additive(x, x)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        encoded = self.encoder(x, attention_weights)\n",
    "\n",
    "        output = encoded.mean(dim=1)\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de entrenamiento del modelo\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    acc_history = {\"train\": [], \"val\": []}\n",
    "    losses = {\"train\": [], \"val\": []}\n",
    "\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data).item()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "            acc_history[phase].append(epoch_acc)\n",
    "            \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, acc_history, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los parámetros del modelo y entrenar\n",
    "#input_dim = input_size * input_size\n",
    "#num_inputs = 1\n",
    "hidden_dim = 256\n",
    "num_classes = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (128) must match the existing size (127) at non-singleton dimension 1.  Target sizes: [1, 128].  Tensor sizes: [127]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m TransformerModel(input_dim\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, num_inputs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, hidden_dim\u001b[39m=\u001b[39;49mhidden_dim, num_classes\u001b[39m=\u001b[39;49mnum_classes) \u001b[39m#input_size=input_size\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[6], line 66\u001b[0m, in \u001b[0;36mTransformerModel.__init__\u001b[0;34m(self, input_dim, num_inputs, hidden_dim, num_classes)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_dim \u001b[39m=\u001b[39m hidden_dim\n\u001b[1;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes \u001b[39m=\u001b[39m num_classes\n\u001b[0;32m---> 66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_positional_encoding(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_inputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_dim)\n\u001b[1;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery_fc \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(input_dim, hidden_dim)\n\u001b[1;32m     69\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys_fc \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(input_dim, hidden_dim)\n",
      "Cell \u001b[0;32mIn[6], line 83\u001b[0m, in \u001b[0;36mTransformerModel.generate_positional_encoding\u001b[0;34m(self, input_size, hidden_dim)\u001b[0m\n\u001b[1;32m     81\u001b[0m     positional_encoding[:, \u001b[39m1\u001b[39m::\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcos(position \u001b[39m*\u001b[39m div_term[:hidden_dim \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m])\n\u001b[1;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# hidden_dim is even\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     positional_encoding[:, \u001b[39m1\u001b[39m::\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcos(position \u001b[39m*\u001b[39m div_term[\u001b[39m1\u001b[39m:hidden_dim \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m])\n\u001b[1;32m     85\u001b[0m \u001b[39mreturn\u001b[39;00m positional_encoding\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (128) must match the existing size (127) at non-singleton dimension 1.  Target sizes: [1, 128].  Tensor sizes: [127]"
     ]
    }
   ],
   "source": [
    "\n",
    "model = TransformerModel(input_dim=3, num_inputs=1, hidden_dim=hidden_dim, num_classes=num_classes) #input_size=input_size\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[8, -1, 256]' is invalid for input of size 3600",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, acc_history, losses \u001b[39m=\u001b[39m train_model(model, dataloaders_dict, criterion, optimizer, num_epochs)\n",
      "Cell \u001b[0;32mIn[92], line 31\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     30\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     32\u001b[0m     _, preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, \u001b[39m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m/anaconda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[91], line 103\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     99\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(batch_size, num_inputs, input_dim, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    101\u001b[0m positional_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_encoding\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m, num_inputs, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(batch_size, num_inputs \u001b[39m*\u001b[39m input_dim, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m positional_encoding\u001b[39m.\u001b[39;49mview(batch_size, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_dim)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[8, -1, 256]' is invalid for input of size 3600"
     ]
    }
   ],
   "source": [
    "model, acc_history, losses = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indices_test = torch.randperm(len(image_datasets['test']))[:int(0.1*len(image_datasets['test']))]\n",
    "\n",
    "test_data_subset = torch.utils.data.Subset(image_datasets['test'], subset_indices_test)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data_subset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    test_loss = running_loss / len(dataloader.dataset)\n",
    "    test_acc = running_corrects / len(dataloader.dataset)\n",
    "\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return test_loss, test_acc, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy, confusion_matrix = evaluate_model(model, test_dataloader)\n",
    "\n",
    "print('Test Loss: {:.4f}, Test Accuracy: {:.4f}'.format(test_loss, test_accuracy))\n",
    "print('Confusion Matrix: \\n{}'.format(confusion_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
