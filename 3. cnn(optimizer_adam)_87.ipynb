{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 285/285 [00:32<00:00,  8.77it/s]\n",
      "5000it [00:22, 227.14it/s]\n",
      "5000it [00:15, 324.47it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root_dir = r'C:\\Users\\User\\Desktop\\UAB\\2nd year\\2nd semester\\neural network-deep learning\\project'  # Ruta del directorio de imágenes\n",
    "\n",
    "N_IDC = []\n",
    "P_IDC = []\n",
    "\n",
    "for dir_name in tqdm(os.listdir(root_dir)):\n",
    "    dir_path = os.path.join(root_dir, dir_name)\n",
    "    if os.path.isdir(dir_path):\n",
    "        negative_dir_path = os.path.join(dir_path, '0')\n",
    "        positive_dir_path = os.path.join(dir_path, '1')\n",
    "        if os.path.isdir(negative_dir_path) and os.path.isdir(positive_dir_path):\n",
    "            negative_image_paths = [\n",
    "                os.path.join(negative_dir_path, image_name)\n",
    "                for image_name in os.listdir(negative_dir_path)\n",
    "                if image_name.endswith('.png')\n",
    "            ]\n",
    "            positive_image_paths = [\n",
    "                os.path.join(positive_dir_path, image_name)\n",
    "                for image_name in os.listdir(positive_dir_path)\n",
    "                if image_name.endswith('.png')\n",
    "            ]\n",
    "            N_IDC.extend(negative_image_paths)\n",
    "            P_IDC.extend(positive_image_paths)\n",
    "\n",
    "total_images = 5000  # Cambiado a 5000 para equilibrar las clases (2500 benignos y 2500 malignos)\n",
    "\n",
    "n_img_arr = np.zeros(shape=(total_images, 50, 50, 3), dtype=np.float32)\n",
    "p_img_arr = np.zeros(shape=(total_images, 50, 50, 3), dtype=np.float32)\n",
    "label_n = np.zeros(total_images)\n",
    "label_p = np.ones(total_images)\n",
    "\n",
    "for i, img in tqdm(enumerate(N_IDC[:total_images])):\n",
    "    n_img = cv2.imread(img, cv2.IMREAD_COLOR)\n",
    "    n_img_size = cv2.resize(n_img, (50, 50), interpolation=cv2.INTER_LINEAR)\n",
    "    n_img_arr[i] = n_img_size\n",
    "\n",
    "for i, img in tqdm(enumerate(P_IDC[:total_images])):\n",
    "    p_img = cv2.imread(img, cv2.IMREAD_COLOR)\n",
    "    p_img_size = cv2.resize(p_img, (50, 50), interpolation=cv2.INTER_LINEAR)\n",
    "    p_img_arr[i] = p_img_size\n",
    "\n",
    "X = np.concatenate((p_img_arr, n_img_arr), axis=0)\n",
    "y = np.concatenate((label_p, label_n), axis=0)\n",
    "X, y = shuffle(X, y, random_state=0)\n",
    "\n",
    "# probar --> y = to_categorical(y)\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, output_size=2):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=4, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout1 = nn.Dropout2d(p=0.3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(12 * 12 * 64, 64)\n",
    "        self.bn5 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "        self.fc3 = nn.Linear(64, 24)\n",
    "        self.fc4 = nn.Linear(24, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "learning_rate = 0.001 #1e-2\n",
    "lambda_l2 = 1e-5\n",
    "momentum = 0.5\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "# probar --> criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=lambda_l2)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    return train_loss\n",
    "\n",
    "def evaluate(model, device, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = 100.0 * correct / len(val_loader.dataset)\n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00, Train Loss: 0.4752, Val Loss: 0.4641, Val Accuracy: 84.33%\n",
      "Epoch: 01, Train Loss: 0.4441, Val Loss: 0.4686, Val Accuracy: 82.78%\n",
      "Epoch: 02, Train Loss: 0.4435, Val Loss: 0.4447, Val Accuracy: 86.39%\n",
      "Epoch: 03, Train Loss: 0.4377, Val Loss: 0.5693, Val Accuracy: 73.06%\n",
      "Epoch: 04, Train Loss: 0.4358, Val Loss: 0.4459, Val Accuracy: 86.11%\n",
      "Epoch: 05, Train Loss: 0.4364, Val Loss: 0.6046, Val Accuracy: 70.72%\n",
      "Epoch: 06, Train Loss: 0.4331, Val Loss: 0.4413, Val Accuracy: 86.44%\n",
      "Epoch: 07, Train Loss: 0.4314, Val Loss: 0.4713, Val Accuracy: 83.67%\n",
      "Epoch: 08, Train Loss: 0.4331, Val Loss: 0.4362, Val Accuracy: 87.11%\n",
      "Epoch: 09, Train Loss: 0.4303, Val Loss: 0.5170, Val Accuracy: 78.06%\n",
      "Epoch: 10, Train Loss: 0.4276, Val Loss: 0.4743, Val Accuracy: 82.11%\n",
      "Epoch: 11, Train Loss: 0.4262, Val Loss: 0.4354, Val Accuracy: 87.06%\n",
      "Epoch: 12, Train Loss: 0.4242, Val Loss: 0.4730, Val Accuracy: 83.33%\n",
      "Epoch: 13, Train Loss: 0.4236, Val Loss: 0.4548, Val Accuracy: 85.50%\n",
      "Epoch: 14, Train Loss: 0.4230, Val Loss: 0.4308, Val Accuracy: 87.72%\n",
      "Epoch: 15, Train Loss: 0.4218, Val Loss: 0.4413, Val Accuracy: 86.11%\n",
      "Epoch: 16, Train Loss: 0.4213, Val Loss: 0.4398, Val Accuracy: 86.61%\n",
      "Epoch: 17, Train Loss: 0.4221, Val Loss: 0.4624, Val Accuracy: 83.83%\n",
      "Epoch: 18, Train Loss: 0.4163, Val Loss: 0.4409, Val Accuracy: 86.72%\n",
      "Epoch: 19, Train Loss: 0.4091, Val Loss: 0.4804, Val Accuracy: 82.89%\n",
      "Epoch: 20, Train Loss: 0.4119, Val Loss: 0.4552, Val Accuracy: 85.50%\n",
      "Epoch: 21, Train Loss: 0.4084, Val Loss: 0.4375, Val Accuracy: 87.22%\n",
      "Epoch: 22, Train Loss: 0.4081, Val Loss: 0.4410, Val Accuracy: 86.72%\n",
      "Epoch: 23, Train Loss: 0.4037, Val Loss: 0.4402, Val Accuracy: 86.61%\n",
      "Epoch: 24, Train Loss: 0.4034, Val Loss: 0.4439, Val Accuracy: 86.61%\n",
      "Epoch: 25, Train Loss: 0.4036, Val Loss: 0.4279, Val Accuracy: 88.33%\n",
      "Epoch: 26, Train Loss: 0.4012, Val Loss: 0.4740, Val Accuracy: 83.33%\n",
      "Epoch: 27, Train Loss: 0.4066, Val Loss: 0.4491, Val Accuracy: 85.78%\n",
      "Epoch: 28, Train Loss: 0.3982, Val Loss: 0.4504, Val Accuracy: 85.83%\n",
      "Epoch: 29, Train Loss: 0.3954, Val Loss: 0.4434, Val Accuracy: 86.61%\n",
      "Epoch: 30, Train Loss: 0.3921, Val Loss: 0.4573, Val Accuracy: 85.28%\n",
      "Epoch: 31, Train Loss: 0.3958, Val Loss: 0.4457, Val Accuracy: 86.33%\n",
      "Epoch: 32, Train Loss: 0.3867, Val Loss: 0.4426, Val Accuracy: 86.83%\n",
      "Epoch: 33, Train Loss: 0.3805, Val Loss: 0.4428, Val Accuracy: 86.83%\n",
      "Epoch: 34, Train Loss: 0.3827, Val Loss: 0.5182, Val Accuracy: 79.33%\n",
      "Epoch: 35, Train Loss: 0.3827, Val Loss: 0.4563, Val Accuracy: 85.28%\n",
      "Epoch: 36, Train Loss: 0.3802, Val Loss: 0.4449, Val Accuracy: 86.50%\n",
      "Epoch: 37, Train Loss: 0.3883, Val Loss: 0.4539, Val Accuracy: 85.61%\n",
      "Epoch: 38, Train Loss: 0.3778, Val Loss: 0.4506, Val Accuracy: 85.78%\n",
      "Epoch: 39, Train Loss: 0.3774, Val Loss: 0.5047, Val Accuracy: 80.44%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batch_size = 32  # Define the batch size\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train).float().permute(0, 3, 1, 2), torch.from_numpy(y_train).long())\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_val).float().permute(0, 3, 1, 2), torch.from_numpy(y_val).long())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_epochs = 40\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, device, train_loader, optimizer, criterion)\n",
    "    val_loss, val_accuracy = evaluate(model, device, val_loader, criterion)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    print('Epoch: {:02d}, Train Loss: {:.4f}, Val Loss: {:.4f}, Val Accuracy: {:.2f}%'.format(\n",
    "        epoch, train_loss, val_loss, val_accuracy))\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, label=\"training loss\")\n",
    "plt.plot(val_loss, label=\"validation loss\")\n",
    "\n",
    "plt.legend()\n",
    "plt.pause(0.000001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4403, Test Accuracy: 87.30%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test).float().permute(0, 3, 1, 2), torch.from_numpy(y_test).long())\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loss, test_accuracy = evaluate(model, device, test_loader, criterion)\n",
    "\n",
    "print('Test Loss: {:.4f}, Test Accuracy: {:.2f}%'.format(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
